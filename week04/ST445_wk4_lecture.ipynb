{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ST445 Managing and Visualizing Data\n",
    "#  Using data from the Internet\n",
    "### Week 4 Lecture, MT 2017 - Kenneth Benoit, Akitaka Matsuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan for today\n",
    "\n",
    "- \n",
    "- Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Web scraping \n",
    "\n",
    "#### What is it?\n",
    "\"Web scraping (web harvesting or web data extraction) is data scraping used for extracting data from websites\" [Wikipedia: Web Scraping](https://en.wikipedia.org/wiki/Web_scraping)\n",
    "\n",
    "\n",
    "![Web Scraping](https://upload.wikimedia.org/wikipedia/commons/d/da/Scrapp.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Web-scraping steps\n",
    "1. Get contents from the web\n",
    "2. Extract information\n",
    "3. Reshape and save the information as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get contents from the web\n",
    "\n",
    "- First of all you need to know where is the information \n",
    "- Examples:\n",
    "    - Government's administrative data\n",
    "    - Newspaper websites\n",
    "- The data format\n",
    "    - web-pages (in html)\n",
    "    - data files in various format (csv, spss, stata)\n",
    "    - document files (MS-Word, pdf)\n",
    "    - API (e.g. JSON)\n",
    "    - pictures\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get to know the target website\n",
    "\n",
    "1. Open the website, learn how it's structured\n",
    "2. \"View page source\" and \"Inspect\"\n",
    "    - [Example 0](http://www.r-datacollection.com/materials/ch-2-html/fortunes.html)\n",
    "    - [Example 1](http://www.r-datacollection.com/materials/ch-6-ajax/fortunes/fortunes1.html)\n",
    "    - [Example 2](http://www.r-datacollection.com/materials/ch-6-ajax/fortunes/fortunes2.html)\n",
    "    - [Example 3](http://www.r-datacollection.com/materials/ch-6-ajax/fortunes/fortunes3.html)\n",
    "\n",
    "These examples looks similar (especially Ex 0 and Ex 2) but the static contents are different, so what a normal scraper can see might be different.\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get webpage contents\n",
    "\n",
    "- Suppose that I you know that what you want to get is in static contents of the webpage (i.e. something you can find in \"View page source\")\n",
    "- Then steps are \n",
    "    1. Get the page contents\n",
    "    2. Parse the contents\n",
    "    3. Extract and format the contents\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get webpage contents in Python    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML//EN\">\\n<html> <head>\\n<title>Collected R wisdoms</title>\\n</head>\\n\\n<body>\\n<div id=\"R Inventor\" lang=\"english\" date=\"June/2003\">\\n  <h1>Robert Gentleman</h1>\\n  <p><i>\\'What we have is nice, but we need something very different\\'</i></p>\\n  <p><b>Source: </b>Statistical Computing 2003, Reisensburg</p>\\n</div>\\n\\n<div lang=\"english\" date=\"October/2011\">\\n  <h1>Rolf Turner</h1>\\n  <p><i>\\'R is wonderful, but it cannot work magic\\'</i> <br><emph>answering a request for automatic generation of \\'data from a known mean and 95% CI\\'</emph></p>\\n  <p><b>Source: </b><a href=\"https://stat.ethz.ch/mailman/listinfo/r-help\">R-help</a></p>\\n</div>\\n\\n<address><a href=\"http://www.rdatacollectionbook.com\"><i>The book homepage</i><a/></address>\\n\\n</body> </html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://www.r-datacollection.com/materials/ch-2-html/fortunes.html\") \n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get webpage contents in R    \n",
    "\n",
    "```r\n",
    "url <- \"http://www.r-datacollection.com/materials/html/fortunes.html\"\n",
    "fortunes <- readLines(con = url)\n",
    "cat(fortunes)\n",
    "## <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML//EN\"> <html> <head> <title>Collected R wisdoms</title> </head>  <body> <div id=\"R Inventor\" lang=\"english\" date=\"June/2003\">   <h1>Robert Gentleman</h1>   <p><i>'What we have is nice, but we need something very different'</i></p>   <p><b>Source: </b>Statistical Computing 2003, Reisensburg </div>  <div lang=english date=\"October/2011\">   <h1>Rolf Turner</h1>   <p><i>'R is wonderful, but it cannot work magic'</i> <br><emph>answering a request for automatic generation of 'data from a known mean and 95% CI'</emph></p>   <p><b>Source: </b><a href=\"https://stat.ethz.ch/mailman/listinfo/r-help\">R-help</a></p> </div>  <address><a href=\"www.r-datacollectionbook.com\"><i>The book homepage</i><a/></address>  </body> </html>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parse html\n",
    "\n",
    "### Typical html structure\n",
    "![html tree](https://www.w3schools.com/js/pic_htmltree.gif)\n",
    "\n",
    "HTML parsers anlayze the structure of html and make it ready for extracting the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTML Parsing\n",
    "\n",
    "The next step is to parse the content of html \n",
    "### A very simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Gentleman\n",
      "Rolf Turner\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://www.r-datacollection.com/materials/ch-2-html/fortunes.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "nameList = bsObj.findAll(\"h1\") # this line extract \"h1\" tags\n",
    "for name in nameList: # this loop print out the content of \"h1\" tags\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XPath\n",
    "\n",
    "You may want to navigate through html structure to get a particular information. \n",
    "\n",
    "####Example\n",
    "- Select in the text of `<i>`-tag inside `<p>`-tag \n",
    "- Select based on the class value (this can be achieved with BeautifulSoup, though)\n",
    "\n",
    "Use `etree` in `lxml`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Gentleman\n",
      "Rolf Turner\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "parser = etree.HTMLParser()\n",
    "tree = etree.parse(\"http://www.r-datacollection.com/materials/ch-2-html/fortunes.html\", parser) \n",
    "h1nodes = tree.xpath('.//div/h1') # find the h1 in div\n",
    "for nod in h1nodes:\n",
    "    print(nod.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolf Turner\n"
     ]
    }
   ],
   "source": [
    "h1nodes_oct2011 = tree.xpath('.//div[@date=\"October/2011\"]/h1') # find the h1 in div with specific date value\n",
    "for nod in h1nodes_oct2011:\n",
    "    print(nod.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R web scraping toolbox\n",
    "\n",
    "-  Get contents\n",
    "    - `RCurl`\n",
    "    - `httr`\n",
    "-  Parse and extract information \n",
    "    - parsing and analyzing markup language:\n",
    "        - `XML`\n",
    "        - `XML2`\n",
    "    - content extraction with matching \n",
    "        - (base R)\n",
    "        - `stringr`\n",
    "        - `sgringi`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python web scraping toolbox\n",
    "\n",
    "-  Get contents\n",
    "    - `urllib`\n",
    "    - `httplib`\n",
    "    - `requests`\n",
    "-  Parse and extract information \n",
    "    - parsing and analyzing markup language:\n",
    "        - `bs4` (`BeautifulSoup`)\n",
    "        - `lxml`\n",
    "    - content extraction with matching \n",
    "        - `re`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selenium\n",
    "\n",
    "- Standard tools for web scraping (e.g. `httr` in R or `urllib` in Python) may not work in some occasions\n",
    "- Reasons:\n",
    "    - \"Some websites donâ€™t like to be webscraped. In these cases you may need to disguise your webscraping bot as a human being. Selenium is just the tool for that.\" [webscraping with Selenium](http://thiagomarzagao.com/2013/11/12/webscraping-with-selenium-part-1/)\n",
    "    - The information is in non-static contents \n",
    "- Solution:\n",
    "    - Use `selenium` = an automated testing suite for web applications \n",
    "    - Manipulate actual web-browser (e.g. Chrome, Firefox) using selenium drivers\n",
    "    \n",
    "WIth selenium, you should be able to get whatever you can get with your browser (theoretically speaking...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caveats\n",
    "\n",
    "#### Web-scraping is not always (or never) welcomed by site-owners\n",
    "\n",
    "#### Why?\n",
    "- excessive traffic \n",
    "- influence on their revenues\n",
    "\n",
    "You can be warned, blocked, and even sued. \n",
    "\n",
    "#### So, what to do?\n",
    "1. Read TOC carefully \n",
    "2. Check `robot.txt` (c.f. http://www.robotstxt.org/)\n",
    "2. Get permission if possible \n",
    "3. Be nice \n",
    "    - place short breaks between fetching\n",
    "    - scrape during off-peak hours\n",
    "    - avoid scraping exessive materials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "#### Python\n",
    "*  [Automate the Boring Stuff with Python](https://automatetheboringstuff.com/)\n",
    "*  [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do)\n",
    "\n",
    "#### R\n",
    "*  [Automated Data Collection with R](http://www.r-datacollection.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coming soon\n",
    "\n",
    "* **Lab**: Simple web-scraping exercise\n",
    "* **Next week**: API"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
